---

layout: post

title: 【Python】分布式训练下常规BN导致的Inplace 错误

date:  2021-08-18 10:00

tags: 

- Python

author: Lixiang Ru

permalink: /2021-08-18-syncbn

mathjax: true

---

在更换网络的backbone时，原本正常运行的代码报错：
``` shell
Traceback (most recent call last):
  File "dist_train_cnn.py", line 415, in <module>
    train(cfg=cfg)
  File "dist_train_cnn.py", line 325, in train
    loss.backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 256, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2048]] is at version 4; expected version 3 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
```
仔细检查发现backbone里面也并没有会引起参数覆盖的操作，几个inplace也都发生在ReLU层，理论上不会导致这个错误。在使用 `with torch.autograd.set_detect_anomaly(True):` 跟踪到问题是：

``` shell
  File "dist_train_cnn.py", line 415, in <module>
    train(cfg=cfg)
  File "dist_train_cnn.py", line 287, in train
    cls, segs = wetr(inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1056, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 855, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1056, in _call_impl
    return forward_call(*input, **kwargs)
  File "/workspace/rulixiang/wetr2/wetr/model_cnn.py", line 149, in forward
    _x = self.encoder(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1056, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1056, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1056, in _call_impl
    return forward_call(*input, **kwargs)
  File "/workspace/rulixiang/wetr2/wetr/deeplab.py", line 51, in forward
    out = self.conv3(out)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1056, in _call_impl
    return forward_call(*input, **kwargs)
  File "/workspace/rulixiang/wetr2/wetr/deeplab.py", line 22, in forward
    x = self.batch_norm(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1056, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 167, in forward
    return F.batch_norm(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 2281, in batch_norm
    return torch.batch_norm(

```

可以看到问题是由于`F.batch_norm`导致的，想到由于这里我使用了分布式训练的设置，会不会是没有将`BN`变成`SyncBN`的原因，更改一下，发现完美解决。:sweat_smile::sweat_smile::sweat_smile:
``` python
class ConvBN(nn.Module):

    def __init__(self, in_planes, out_planes, kernel_size=1, stride=1, padding=0, dilation=1, batch_norm=None):
        super(ConvBN, self).__init__()

        ''''''

        if batch_norm is None:
            #self.batch_norm = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=1e-3)
            self.batch_norm = nn.SyncBatchNorm(out_planes, eps=1e-5, momentum=1e-3)
```