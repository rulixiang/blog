---
layout: post
title:  Tensorflow 自定义层和梯度
date:   2019-09-29 10:00
tags: 
- Tensorflow
author: Lixiang Ru
permalink: /2019-09-29-tf_grad
mathjax: true
---

最近在尝试一个网络优化的问题，由于目标函数中包含了SVD分解，协方差矩阵计算，矩阵求逆等一系列运算，所以用tensorflow自动的过程中会出现令人很不愉快的结果，最常见的结果是优化过程出现NaN。实际上，这个问题已经困扰了我有一段时间。

现在打算尝试的方法是手动求导修改梯度，所以要学习一下tensorflow进行自定义层和梯度的过程。

现在假如我们要对一个简单的多元函数求导，例如
```
y = x1 + x2
```
$$x1, x2$$ 是函数的两个输入，乘积对他们的梯度均为全1矩阵，我们下面对其进行自定义梯度的修改，分别写改成$$x1, x2$$.

``` python

import tensorflow as tf 

a = tf.Variable([[1.,2.],[1.,2.]], name='a')
b = tf.Variable([[2.,3.],[2.,3.]], name='b')

## 原始自动求得梯度
y1 = tf.add(a,b)
grads_a1 = tf.gradients(y1, a)
grads_b1 = tf.gradients(y1, b)

## 自定义回传梯度
@tf.custom_gradient
def my_func(inputs):

    x1 = inputs[0:2,:]
    x2 = inputs[2:,:]

    def grad(dx):
        
        return tf.concat((x1,x2), axis=0)

    return tf.add(x1,x2), grad

ab = tf.concat((a,b), axis=0)
y2 = my_func(ab)
grads_2 = tf.gradients(y2, ab)

sess=tf.Session()
sess.run(tf.global_variables_initializer())

print('inputs a is:')
print(sess.run(a))
print('inputs b is:')
print(sess.run(b))

print('original grads for a is:')
print(sess.run(grads_a1))
print('original grads for b is:')
print(sess.run(grads_b1))
  
print('modified grads for a is:')
print(sess.run(grads_2))
print('modified grads for b is:')
print(sess.run(grads_2))

```

运行输出：

``` shell
inputs a is:
[[1. 2.]
 [1. 2.]]
inputs b is:
[[2. 3.]
 [2. 3.]]
original grads for a is:
[array([[1., 1.],
       [1., 1.]], dtype=float32)]
original grads for b is:
[array([[1., 1.],
       [1., 1.]], dtype=float32)]
modified grads for a is:
[array([[1., 2.],
       [1., 2.],
       [2., 3.],
       [2., 3.]], dtype=float32)]
modified grads for b is:
[array([[1., 2.],
       [1., 2.],
       [2., 3.],
       [2., 3.]], dtype=float32)]
```